---
title: "Titanic"
author: "Christian Álvarez Peláez"
date: "9 de octubre de 2020"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: false
    toc_depth: 1
    #code_folding: hide
---
In building...

```{r include=FALSE}
library(reticulate)
```


```{python}
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier

```


```{python}

# Load in the train and test trains
train = pd.read_csv('Titanic/train.csv')

train.head(3)
train.shape

# list(data) or 
list(train.columns) 
```

```{python}
train['Ticket_type'] = train['Ticket'].apply(lambda x: x[0:3])
train['Ticket_type'] = train['Ticket_type'].astype('category')
train['Ticket_type'] = train['Ticket_type'].cat.codes
```

```{python}
# Feature that tells whether a passenger had a cabin on the Titanic
train['Has_Cabin'] = train["Cabin"].apply(lambda x: 0 if type(x) == float else 1)


# Feature engineering steps taken from Sina
# Create new feature FamilySize as a combination of SibSp and Parch
train['FamilySize'] = train['SibSp'] + train['Parch'] + 1

# Create new feature IsAlone from FamilySize
train['IsAlone'] = 0
train.loc[train['FamilySize'] == 1, 'IsAlone'] = 1
# Remove all NULLS in the Embarked column
train['Embarked'] = train['Embarked'].fillna('S')
# Remove all NULLS in the Fare column and create a new feature CategoricalFare
train['Fare'] = train['Fare'].fillna(train['Fare'].median())
train['CategoricalFare'] = pd.qcut(train['Fare'], 4)
# Create a New feature CategoricalAge
age_avg = train['Age'].mean()
age_std = train['Age'].std()
age_null_count = train['Age'].isnull().sum()
age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)
train['Age'][np.isnan(train['Age'])] = age_null_random_list
train['Age'] = train['Age'].astype(int)
train['CategoricalAge'] = pd.cut(train['Age'], 5)

# Mapping Sex
train['Sex'] = train['Sex'].map( {'female': 0, 'male': 1} ).astype(int)
    
# Mapping Embarked
train['Embarked'] = train['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)
  
# Mapping Fare
train.loc[ train['Fare'] <= 7.91, 'Fare'] 						        = 0
train.loc[(train['Fare'] > 7.91) & (train['Fare'] <= 14.454), 'Fare'] = 1
train.loc[(train['Fare'] > 14.454) & (train['Fare'] <= 31), 'Fare']   = 2
train.loc[ train['Fare'] > 31, 'Fare'] 							        = 3
train['Fare'] = train['Fare'].astype(int)
    
# Mapping Age
train.loc[ train['Age'] <= 16, 'Age']= 0
train.loc[(train['Age'] > 16) & (train['Age'] <= 32), 'Age'] = 1
train.loc[(train['Age'] > 32) & (train['Age'] <= 48), 'Age'] = 2
train.loc[(train['Age'] > 48) & (train['Age'] <= 64), 'Age'] = 3
train.loc[ train['Age'] > 64, 'Age'] = 4 ;

```

```{python}
#Split the train

X=train[ ['Sex', 'Age', 'Parch', 'Fare', 'Embarked', 'Ticket_type', 'Has_Cabin', 'FamilySize', 'IsAlone']]
y=train[['Survived']]


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)

```



```{python}
lr = LogisticRegression(C=100.0, random_state=1,solver='lbfgs', multi_class='ovr')
lr.fit(X_train, y_train)

```



```{python include=FALSE}
scoreslr = cross_val_score(estimator=lr,X=X_train,y=y_train,cv=10,n_jobs=1)
```

```{python}
print('CV accuracy: %.3f +/- %.3f' % (np.mean(scoreslr),np.std(scoreslr)))
lrweight=np.mean(scoreslr)
Weight=np.array([lrweight])
```



```{python}
svm = SVC(kernel='rbf', random_state=1, gamma=0.10, C=10.0)
svm.fit(X_train, y_train)

```


```{python include=FALSE}
scoressvm = cross_val_score(estimator=svm,X=X_train,y=y_train,cv=10,n_jobs=1)
```

```{python}
print('CV accuracy: %.3f +/- %.3f' % (np.mean(scoressvm),np.std(scoressvm)))
svmWeight=np.mean(scoressvm)
Weight=np.append(Weight,svmWeight)
```




```{python}

forest = RandomForestClassifier(criterion='gini', n_estimators=25, random_state=1)
forest.fit(X_train, y_train)

```


```{python include=FALSE}
scoresfo = cross_val_score(estimator=forest,X=X_train,y=y_train,cv=10,n_jobs=1)
```

```{python}
print('CV accuracy: %.3f +/- %.3f' % (np.mean(scoresfo),np.std(scoresfo)))

forestWeight=np.mean(scoresfo)
Weight=np.append(Weight,forestWeight)

```



```{python}

knn = KNeighborsClassifier(n_neighbors=5, p=2,metric='minkowski')
knn.fit(X_train, y_train)

```


```{python include=FALSE}
scoresk = cross_val_score(estimator=knn,X=X_train,y=y_train,cv=10,n_jobs=1)
```

```{python}
print('CV accuracy: %.3f +/- %.3f' % (np.mean(scoresk),np.std(scoresk)))
knnWeight=np.mean(scoresk)
Weight=np.append(Weight,knnWeight)

```



```{python}

from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

tree = DecisionTreeClassifier(criterion='entropy',random_state=1,max_depth=1)
ada = AdaBoostClassifier(base_estimator=tree,n_estimators=500,learning_rate=0.1,random_state=1)
ada.fit(X_train, y_train)

``` 


```{python include=FALSE}
scoresAda = cross_val_score(estimator=ada,X=X_train,y=y_train,cv=10,n_jobs=1)
```

```{python}
print('CV accuracy: %.3f +/- %.3f' % (np.mean(scoresAda),np.std(scoresAda)))
adaWeight=np.mean(scoresAda)
Weight=np.append(Weight,adaWeight)

```


```{python}
pred=(Weight[0]*lr.predict(X_test)+Weight[1]*svm.predict(X_test)+Weight[2]*forest.predict(X_test)+Weight[3]*knn.predict(X_test)+Weight[4]*ada.predict(X_test))/np.sum(Weight)

pred=np.round(pred)

pred.shape
y_real=y_test.values.reshape(179,)
s=y_real-pred
1-np.mean(np.absolute(s))
```