---
title: "Aplication of functional data to MNIST"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: false
    toc_depth: 1
    #code_folding: hide
---



# Voice to image with functional regression

2020 | 07 | 23 Last compiled: `r Sys.Date()`

In this section our focus will be in find a way to implement the functional regression with functional response to the MNIST problem. Our goal will be to a make a model where the response is a handwritte number and the covariable a voice note of the number. Due to the computational cost of this model, we will focus on a simple case, using only the number zero.

##The voice data.

The first step in our analysis will be to study the behavior of our audio notes. We will represent two voice note using functional data.

```{r message=FALSE, warning=FALSE}
library(seawaveQ)
library(audio)
library(fda.usc)
library(stats)
```

```{r}
#Loading the data audio of the spoken numbers
s0<-load.wave(where = "spoken/datos/0_jackson_0.wav")
s1<-load.wave(where = "spoken/datos/0_jackson_1.wav")

sc<-cbind(s0,s1)

#Making the functional data

nbx<-291
basisx <- create.fourier.basis(c(0,5148),nbasis = nbx )
fds<-Data2fd((sc),seq(0,5148,length=5148),basisobj = basisx)

par(mfrow=c(1,2))
plot(fds,main="Functional data approach",xlab="Time",
        ylab="Amplitude")
plot.ts(sc[,1],main="Real data",xlab="Time",
        ylab="Amplitude")
lines(sc[,2], col = "red")

```


We have used a 291 elements of the Fourier basis. It is a very large number of elements, and the approximation isn't as good as we expect. The problem here is that our data is very rough. We will make a discrete Fourier transform of the data (we want to work in frequency domain) to make easier the approximation. We will also work in absolute value for ease.

```{r warning=FALSE}
#We make a fast discrete fourier transform 
s0t<-fft(s0)
s0t<-abs(s0t)
s1t<-fft(s1)
s1t<-abs(s1t)


sct<-cbind(s0t,s1t)

#Making the functional data

nbx<-291
basisx <- create.fourier.basis(c(0,5148),nbasis = nbx )
fdst<-Data2fd((sct),seq(0,5148,length=5148),basisobj = basisx)

par(mfrow=c(1,2))
plot(fdst,main="Functional data approach",xlab="Time",
        ylab="Amplitude")
plot.ts(sct[,1],main="Real data",xlab="Time",
        ylab="Amplitude")
lines(sct[,2], col = "red")

```

As we can see, the approximation is much better than the first attempt.


```{r warning=FALSE}

temp = list.files(pattern="*wav")
myfiles = lapply(temp, load.wave)

M<-matrix(data=0,nrow = 50,ncol = 6273)
for (i in 1:50) {
 
  M[i,1:length(myfiles[[i]])]<-myfiles[[i]]
    
}

#Our functional covariable

x<-abs(t(fft(M)))
nbx<-51
basisx <- create.fourier.basis(c(0,6273),nbasis = nbx )
SFD<-Data2fd((x),seq(0,6273,length=6273),basisobj = basisx)


```

##The MNIST dataset.

The following image shows a set of handwritten  numbers from the MNIST data set.

![](images/mnist.png)



The dataset has 785 colums. The firts one indicates the value of the handwrite number and the next 784 the value of the pixels. This pixel form a 28x28 image.

```{r}
train<-read.csv(file = "spoken/datos/train.csv")

```

We only will work with the number 0. Since we just have 50 voice note  of the number 0, we will only take 50 random 0 from the dataset.

```{r warning=FALSE}
set.seed(12345)
ii<-which(train$label==0)
index<-sample(ii,50)
ll<-sample(1:50,10)

#Our functional response model 

y <-t(train[index,2:ncol(train)])
nby<-181
basisy<-  create.fourier.basis(c(0,784),nbasis = nby )
DFD<-Data2fd((y),seq(0,784,length=784),basisobj = basisy)



```

##Functional reggression with functional response.

The model we will use will be the one proposed by Ramsay and Silverman. It is a penalized functional regression. This penalty is necessary to avoid the problem of sparsity, because the estimation of the model depends on determining the inverter of a matrix whose dimension is the number of elements of the base chosen. The greater this number of elements, the more difficult it will be to find an inverse of this matrix. The model has the form:

$$Y_{i}(t)=\alpha(t)+\int \beta(t,s)X_{i}(s)ds+\epsilon_{i}(t)$$

The model estimate is given as:

$$vec(\mathbf{\widehat{B}})=[J_{\theta \theta}\otimes(\mathbf{X}'\mathbf{X})+\lambda_{s}J_{\theta \theta}\otimes\mathbf{R}+\lambda_{t}\mathbf{Q}\otimes J_{\eta \eta}]^{-1}(J_{\phi \theta}\otimes \mathbf{X}')vec(\mathbf{C})$$
where 

$$[J_{\theta \theta}\otimes (\mathbf{X}' \mathbf{X} )]vec(\mathbf{B} )=vec\left(\mathbf{X}'  \int Y(t)\theta ' (t) dt \right) $$


Now we are going to implement all this ideas

```{r warning=FALSE}
lambdas<-1
lambdat<-1

tini <- proc.time()
res.fr = fregre.basis.fr(datosf[-ll],datosfN[-ll],basisx,basisy,lambda.s = lambdas,lambda.t = lambdat,Lfdobj.s = 1,Lfdobj.t = 1)
proc.time() - tini
prf<-predict(res.fr,datosf[ll],basisobj = basisy)
pr<-eval.fd(prf,seq(0,784,length=784))

```

To determine if our model is capable of creating handwritten number images, we will only draw the output of our model using the test set.


```{r warning=FALSE}

A1<-matrix(as.double(pr[,4]),nrow = 28,byrow = T)
rotate <- function(x) t(apply(x, 2, rev))
image(rotate(A1))


#Making the pixels that have values less than 30 be 0.
jj<-which(pr[,4]<50)
pr[jj,4]=0
A1<-matrix(as.double(pr[,4]),nrow = 28,byrow = T)
image(rotate(A1))
```



The results are not entirely good, but we can clearly see that it is a 0. On the other hand, it should be noted that we are only working with a very low number of base elements. As we increase the number of elements, the resolution of the image also increases.



#MNIST classification problem with functional data.


```{r message=FALSE, warning=FALSE}
data<-read.csv(file = "spoken/datos/train.csv",sep = ",")
datap<-factor(data$label[1:4000])
fdatap<-fdata(data[1:4000,2:ncol(data)])
ftestp<-fdata((data[31000:31500,2:ncol(data)]))
valuesp<-factor(data$label[31000:31500])
ldatap=list(df=data.frame(ipho=datap),Xpho=fdatap)
ldatanew=list(df=data.frame(ipho=valuesp),Xpho=ftestp)
```


```{r}
opt.pc=fdata2pc(fdatap,ncomp=2)
pcb<-round(opt.pc$d^2/sum(opt.pc$d^2),5) #Porc. de variabilidad
sum(pcb[1:35])
b.xp=list(Xpho=create.pc.basis(fdatap,1:35))
```

```{r}
#Modelo SVM
t<-proc.time()
pho.svm<-classif.svm(ipho~Xpho,ldatap,basis.x = b.xp)
proc.time() - t
pr.svm=predict(pho.svm,ldatanew)
bb<-table(ldatanew$df$ipho,pr.svm)
bb

psvm<-0
for (i in 1:10) {
  psvm[i]<-bb[i,i]/sum(bb[i,])
}
psvm
mean(psvm)
```

```{r}
#rf
set.seed(12345)
t<-proc.time()
pho.rf<-classif.randomForest(ipho~Xpho,ldatap,basis.x=b.xp)
proc.time() - t
pr.rf=predict(pho.rf,ldatanew)
bb<-table(ldatanew$df$ipho,pr.rf)
bb

pgrf<-0
for (i in 1:10) {
  pgrf[i]<-bb[i,i]/sum(bb[i,])
}
pgrf
mean(pgrf)
```




I wrote an [introductory programming book that introduces R](https://crumplab.github.io/programmingforpsych/), and gives some [concrete problems for you to solve](https://crumplab.github.io/programmingforpsych/programming-challenges-i-learning-the-fundamentals.html). 









